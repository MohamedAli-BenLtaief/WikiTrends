services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - core

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    networks:
      - core
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "kafka:9092", "--list"]
      interval: 10s
      retries: 5

  ingestor:
    build:
      context: ./ingestor
    container_name: ingestor
    depends_on:
      - kafka
    networks:
      - core
    restart: unless-stopped

  streamer:
    build:
      context: ./streamer
    container_name: streamer
    depends_on:
      - kafka
    networks:
      - core
    restart: unless-stopped
    command: /opt/spark/bin/spark-submit --master local[*] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 /app/streamer.py
    user: root
  
  opensearch:
    image: opensearchproject/opensearch:2.14.0
    container_name: opensearch
    environment:
      discovery.type: single-node
      DISABLE_SECURITY_PLUGIN: "true"
      OPENSEARCH_JAVA_OPTS: "-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
    volumes:
      - opensearch-data:/usr/share/opensearch/data
      - ./conf/opensearch/index-templates:/conf/opensearch/index-templates:ro
    networks:
      - core
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.14.0
    container_name: opensearch-dashboards
    depends_on:
      opensearch:
        condition: service_healthy
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch:9200"]'
      DISABLE_SECURITY_DASHBOARDS_PLUGIN: "true"
    ports:
      - "5601:5601"
    networks:
      - core
    
  indexer:
    build: ./indexer
    container_name: indexer
    depends_on:
      kafka:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    networks:
      - core
    restart: unless-stopped
  
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    environment:
      CLUSTER_NAME: wikimedia-cluster
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: "false"
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
    networks:
      - core
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 10s
      timeout: 5s
      retries: 10

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
    ports:
      - "9864:9864"
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    networks:
      - core

  hbase:
    image: harisekhon/hbase:2.1
    container_name: hbase
    hostname: hbase
    environment:
      HBASE_MANAGES_ZK: "true"
    ports:
      - "16010:16010"
      - "16030:16030"
      - "8085:8085"
      - "9090:9090"
    networks:
      - core
    healthcheck:
      test: ["CMD-SHELL", "grep -q 'Registered as active master' /hbase/logs/hbase--master-hbase.log"]
      interval: 10s
      timeout: 5s
      retries: 40
      start_period: 90s
  
  writer:
    build: ./writer
    container_name: writer
    depends_on:
      kafka:
        condition: service_healthy
      hbase:
        condition: service_healthy
    networks:
      - core
    restart: unless-stopped

volumes:
  opensearch-data:
  hadoop-namenode:
  hadoop-datanode:

networks:
  core:
    driver: bridge